{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import all the necessary packages"
      ],
      "metadata": {
        "id": "e-wVwMyVKMxZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARjLjxRGJ-vU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from ntlk.stem import WordNetLemmatizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertModel\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from bayes_opt import BayesianOptimization\n",
        "import autokeras as ak\n",
        "from fastai.tabular.all import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset files"
      ],
      "metadata": {
        "id": "5rxMxAx8KFvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_traces = pd.read_csv('Event_traces.csv')\n",
        "log_templates = pd.read_csv('HDFS.log_templates.csv')\n",
        "anomaly_labels = pd.read_csv('anomaly_label.csv')\n",
        "event_occurence = pd.read_csv('Event_occurence_matrix.csv')"
      ],
      "metadata": {
        "id": "MGZL9b9IKgdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 : Data Integration, Feature engineering, Text Data Preprocessing and Feature Combination"
      ],
      "metadata": {
        "id": "9fYm6hT6WJnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge relevant datasets using BlockID as key"
      ],
      "metadata": {
        "id": "bUTFOfZwLDeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = event_traces.merge(anomaly_labels, on='BlockID')\n",
        "merged_data = merged_data.merge(event_occurences, on='BlockID')\n",
        "merged_data = merged_data.merge(log_template, on='EventID')"
      ],
      "metadata": {
        "id": "sKSsd3oVLHkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "SpDyMhWzLbbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Block Attributes"
      ],
      "metadata": {
        "id": "8bfaQY2HOzl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Block attributes including one-hot encoding for categorical 'Label'"
      ],
      "metadata": {
        "id": "-sDpmDzISykW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_attributes = merged_data[['Time Interval', 'Latency']]\n",
        "label_encoder = OneHotEncoder(sparse=False)\n",
        "encoded_labels = label_encoder.fit_transform(merged_data[['Label']])\n",
        "block_attributes = pd.concat([block_attributes, pd.DataFrame(encoded_labels, columns = label_encoder.get_feature_names(['Label']))], axis = 1)"
      ],
      "metadata": {
        "id": "QPTouKakS8uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 One-hot encoding for categorical 'Type' attribute"
      ],
      "metadata": {
        "id": "DxPxABNtUKwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type_encoder = OneHotEncoder(sparse=False)\n",
        "encoded_types = type_encoder.fit_transform(merged_data[['Type']])\n",
        "block_attributes = pd.concat([block_attributes, pd.DataFrame(encoded_types, columns = type_encoder.get_feature_names(['Type']))], axis = 1)"
      ],
      "metadata": {
        "id": "1P7KomULUSQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Log Templates"
      ],
      "metadata": {
        "id": "2eXMnkaqPDpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = merged_data['EventTemplate']"
      ],
      "metadata": {
        "id": "TYSILhK8PF7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Text Data Processing"
      ],
      "metadata": {
        "id": "xf8ZoCjjPNC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "xUtVlPoGPP23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tokenization and lowercasing"
      ],
      "metadata": {
        "id": "TJyF-nTpPaMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = teaxt_data.apply(lambda x : nltk.word_tokenize(x.lower()))"
      ],
      "metadata": {
        "id": "XFxUAo-oPddx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Stop words removal"
      ],
      "metadata": {
        "id": "tLU7b-A3PmMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "text_data = text_data.apply(lambda x : [word for word in x if word not in stop_words])"
      ],
      "metadata": {
        "id": "62erahcMPpZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Lemmatization"
      ],
      "metadata": {
        "id": "YdGVneyuP-AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "text_data = text_data.apply(lambda x : [lemmatizer.lemmatize(word) for word in x])"
      ],
      "metadata": {
        "id": "Pot_PK1cQBS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert back to text"
      ],
      "metadata": {
        "id": "iKMdRr81QOT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = text_data.apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "V9xPdOIJQQpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine Features"
      ],
      "metadata": {
        "id": "9I7HRQadQZsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize text data"
      ],
      "metadata": {
        "id": "c36dyqBSQcPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features = 1000)\n",
        "text_features = vectorizer.fit_transform(text_data).toarray()"
      ],
      "metadata": {
        "id": "_xFh72i3QfBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize numerical attributes"
      ],
      "metadata": {
        "id": "fWdD3HPHQq30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "num_attributes = scaler.fit_transform(block_attributes)"
      ],
      "metadata": {
        "id": "MP4rn2NrQ7Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine text and numerical features"
      ],
      "metadata": {
        "id": "IKS1LNfzRHI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_features = pd.concat([pd.DataFrame(text_features), pd.DataFrame(num_attributes)], axis = 1)"
      ],
      "metadata": {
        "id": "1L0ym0PjRKMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting"
      ],
      "metadata": {
        "id": "ECSF-GPyRXcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training, validation and testing sets(80%, 10%, 10%)"
      ],
      "metadata": {
        "id": "M9HmIN8-YFDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split (\n",
        "    combined_features, merged_data['Label'], test_size=0.2, random_state=42, stratify=merged_data['Label']\n",
        ")"
      ],
      "metadata": {
        "id": "2Mj8i4rgYM35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")"
      ],
      "metadata": {
        "id": "4Sgzq_3BYi08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the shapes of the split datasets"
      ],
      "metadata": {
        "id": "LmuZ0ue8YxYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data shape: \", X_train.shape)\n",
        "print(\"Validation data shape: \", X_val.shape)\n",
        "print(\"Testing data shape: \", X_test.shape)"
      ],
      "metadata": {
        "id": "-8JUy3PiY0X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 : Model Selection and Development"
      ],
      "metadata": {
        "id": "aTaJ9BdiZMjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I. Prepare the tabular data"
      ],
      "metadata": {
        "id": "RVsxI9feVi1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "seperate features and labels"
      ],
      "metadata": {
        "id": "oQk_FNYIVmH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tabular = X_train.iloc[:,1000:]\n",
        "X_val_tabular = X_val.iloc[:,1000:]\n",
        "y_train_tabular = y_train\n",
        "y_val_tabular = y_val"
      ],
      "metadata": {
        "id": "oKq3GQKbZPle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparamter tuning for XGBoost"
      ],
      "metadata": {
        "id": "3xk4CyPOV6bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a ColumnTransformer for numerical and categorical features"
      ],
      "metadata": {
        "id": "mT8_vr7uWABs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = X_train_tabular.columns[:2]\n",
        "categorical_features = X_train_tabular.columns[2:]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers = [\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "gMKCKgDdWFZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define XGBoost model"
      ],
      "metadata": {
        "id": "3SfjGm6qWu9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")"
      ],
      "metadata": {
        "id": "lRI2xF7yWytP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a pipeline"
      ],
      "metadata": {
        "id": "7A9F_eGdW_aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model',xgb_model)])"
      ],
      "metadata": {
        "id": "OZq0UU9HXBuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Hyperparameters to tune"
      ],
      "metadata": {
        "id": "3MaWyaBOXMJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid={\n",
        "    'model__n_estimators': [50, 100, 150],\n",
        "    'model__max_depth': [4, 6, 8],\n",
        "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "}"
      ],
      "metadata": {
        "id": "fFI-zy4rXPWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GridSearchCV for hyperparameter tuning"
      ],
      "metadata": {
        "id": "-A_Fz4m6XXsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fix(X_train_tabular, y_train_tabular)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters : \", best_params)\n",
        "\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "7iNF_ZnCXc2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaulate the Tuned XGBoost Model"
      ],
      "metadata": {
        "id": "UVOXmT7bX5sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on the validation set\n",
        "y_pred_val_tabular_tuned = best_model.predict(X_val_tabular)"
      ],
      "metadata": {
        "id": "AVP8j7-fX-wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating accuracy"
      ],
      "metadata": {
        "id": "7GUijbBjYJKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_accuracy = accuracy_score(y_Val_tabular, y_pred_val_tabular_tuned)\n",
        "print(\"Validation Accuracy (Tuned XGBoost) : \", tuned_accuracy)"
      ],
      "metadata": {
        "id": "NAikKmUMYNFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "II. Text Classification Models"
      ],
      "metadata": {
        "id": "UmgSty9RaDNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Text Data using BERT Tokenizer\n",
        "tokenize = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_Case=True)"
      ],
      "metadata": {
        "id": "p5Xnz4YRaIY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text and convert to input IDs and attention masks\n",
        "encoded_text = tokenizer.batch_encode_plus(\n",
        "    text_data,\n",
        "    add_special_tokens=Ture,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "input_ids = encoded_text['input_ids']\n",
        "attention_mask = encoded_text['attention_mask']"
      ],
      "metadata": {
        "id": "hKF5S_1razCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to tensors\n",
        "y_train_text = torch.tensor(y_train)\n",
        "y_Val_text = torch.tensor(y_val)"
      ],
      "metadata": {
        "id": "oTkneQk6bV0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorDatasets for text data\n",
        "train_dataset = TensorDataset(input_ids, attention_mask, y_train_text)\n",
        "val_dataset = TensorDataset(input_ids, attention_mask, y_val_text)"
      ],
      "metadata": {
        "id": "3eWLdamXbg_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DataLoader for batching\n",
        "train_batch_size = 16  # Adjust as needed\n",
        "val_batch_size = 16  # Adjust as needed\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "rfQbtMsPb64d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pre-trained BERT Model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Optimizer and Loss Function\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 3  # Adjust as needed\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, attention_mask, labels = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg. Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "hiTjlax7b9U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on Validation Set\n",
        "model.eval()\n",
        "y_pred_text = []\n",
        "y_true_text = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, attention_mask, labels = batch\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        y_pred_text.extend(preds.cpu().numpy())\n",
        "        y_true_text.extend(labels.cpu().numpy())\n",
        "\n",
        "val_accuracy = accuracy_score(y_true_text, y_pred_text)\n",
        "print(\"Validation Accuracy (BERT):\", val_accuracy)"
      ],
      "metadata": {
        "id": "DsYaUEXfcN_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "III. Time Series Analysis"
      ],
      "metadata": {
        "id": "T46wJMrdqNtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a time series dataset from the 'Time Interval' and 'Latency' attributes\n",
        "time_series_data = merged_data[['Time Interval', 'Latency']].copy()"
      ],
      "metadata": {
        "id": "l-LZ3c84qQTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the time series data\n",
        "time_series_data['Time Interval'] = (time_series_data['Time Interval'] - time_series_data['Time Interval'].min()) / (time_series_data['Time Interval'].max() - time_series_data['Time Interval'].min())\n",
        "time_series_data['Latency'] = (time_series_data['Latency'] - time_series_data['Latency'].min()) / (time_series_data['Latency'].max() - time_series_data['Latency'].min())"
      ],
      "metadata": {
        "id": "PNXTnQoDqdDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences and target values for LSTM\n",
        "sequence_length = 10  # Adjust as needed\n",
        "\n",
        "input_sequences = []\n",
        "target_values = []\n",
        "\n",
        "for i in range(len(time_series_data) - sequence_length):\n",
        "    input_seq = time_series_data.iloc[i:i+sequence_length]['Latency'].values\n",
        "    target_value = time_series_data.iloc[i+sequence_length]['Latency']\n",
        "\n",
        "    input_sequences.append(input_seq)\n",
        "    target_values.append(target_value)\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "target_values = np.array(target_values)"
      ],
      "metadata": {
        "id": "JnvzeKJWql0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the time series data into training and validation sets\n",
        "X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(input_sequences, target_values, test_size=0.2, random_state=42, shuffle=False)"
      ],
      "metadata": {
        "id": "BRVuYm_Qqp4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(sequence_length, 1)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "FBKmvm8Hqt-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the LSTM model\n",
        "model.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=16, validation_data=(X_val_lstm, y_val_lstm))\n"
      ],
      "metadata": {
        "id": "fvnxoEAbq2Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the LSTM model on validation data\n",
        "y_pred_lstm = model.predict(X_val_lstm)\n",
        "mse_lstm = mean_squared_error(y_val_lstm, y_pred_lstm)\n",
        "print(\"Mean Squared Error (LSTM):\", mse_lstm)"
      ],
      "metadata": {
        "id": "ySoZ07Yrq3OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual vs. predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_val_lstm, label='Actual Latency')\n",
        "plt.plot(y_pred_lstm, label='Predicted Latency')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Latency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G8Az1okcq60C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IV. Multi Modal Approach"
      ],
      "metadata": {
        "id": "aS5rBg8a0Rwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the BERT model for text data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "3d8rOZrO0WZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text and convert to input IDs and attention masks\n",
        "encoded_text = tokenizer.batch_encode_plus(\n",
        "    text_data,\n",
        "    add_special_tokens=True,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        "    max_length=128  # Adjust as needed\n",
        ")\n",
        "\n",
        "input_ids = encoded_text['input_ids']\n",
        "attention_mask = encoded_text['attention_mask']"
      ],
      "metadata": {
        "id": "2819OKjV0dNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to tensors\n",
        "y_train_text = torch.tensor(y_train)\n",
        "y_val_text = torch.tensor(y_val)"
      ],
      "metadata": {
        "id": "AUlczAcB0hTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorDatasets for text data\n",
        "train_dataset = TensorDataset(input_ids, attention_mask, y_train_text)\n",
        "val_dataset = TensorDataset(input_ids, attention_mask, y_val_text)"
      ],
      "metadata": {
        "id": "YwxfTzJu04xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DataLoader for batching\n",
        "train_batch_size = 16  # Adjust as needed\n",
        "val_batch_size = 16  # Adjust as needed\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "8YJGLRE205kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adaptation: Prepare structured features and labels\n",
        "train_loader_structured = torch.tensor(X_train)  # Adjust based on your structured data\n",
        "val_loader_structured = torch.tensor(X_val)      # Adjust based on your structured data\n",
        "train_loader_labels = torch.tensor(y_train)      # Adjust based on your labels\n",
        "val_loader_labels = torch.tensor(y_val)          # Adjust based on your labels"
      ],
      "metadata": {
        "id": "N2KGShrE0_yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a neural network for fusion\n",
        "class MultiModalFusion(nn.Module):\n",
        "    def __init__(self, text_embedding_dim, num_structured_features, hidden_dim, output_dim):\n",
        "        super(MultiModalFusion, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.xgboost_fc = nn.Linear(num_structured_features, hidden_dim)\n",
        "        self.fusion_fc = nn.Linear(text_embedding_dim + hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, text_input_ids, text_attention_mask, structured_features):\n",
        "        text_outputs = self.bert(text_input_ids, attention_mask=text_attention_mask)[0]\n",
        "        text_pooled_output = torch.mean(text_outputs, dim=1)\n",
        "\n",
        "        xgboost_features = self.xgboost_fc(structured_features)\n",
        "\n",
        "        fusion_input = torch.cat((text_pooled_output, xgboost_features), dim=1)\n",
        "        fusion_input = self.relu(fusion_input)\n",
        "        fusion_input = self.dropout(fusion_input)\n",
        "\n",
        "        output = self.fusion_fc(fusion_input)\n",
        "        return output"
      ],
      "metadata": {
        "id": "je-GKZDW1Dup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and set up the model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "multi_modal_model = MultiModalFusion(text_embedding_dim=768, num_structured_features=combined_features.shape[1], hidden_dim=128, output_dim=2)\n",
        "multi_modal_model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(multi_modal_model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5, verbose=True)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "8msQ80ro1f2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with early stopping\n",
        "epochs = 10  # Adjust as needed\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_patience = 5\n",
        "early_stopping_counter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    multi_modal_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_text, batch_structured, batch_labels in zip(train_loader, train_loader_structured, train_loader_labels):\n",
        "        batch_text = tuple(t.to(device) for t in batch_text)\n",
        "        batch_structured = batch_structured.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = multi_modal_model(*batch_text, batch_structured)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg. Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    multi_modal_model.eval()\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_text, batch_structured, batch_labels in zip(val_loader, val_loader_structured, val_loader_labels):\n",
        "            batch_text = tuple(t.to(device) for t in batch_text)\n",
        "            batch_structured = batch_structured.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            outputs = multi_modal_model(*batch_text, batch_structured)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            val_predictions.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ],
      "metadata": {
        "id": "4uHajbv_1kqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V. Deep Learning Architecture"
      ],
      "metadata": {
        "id": "TkuKoz2F4-pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the BERT model for text data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "l_ijIprW5FSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text and convert to input IDs and attention masks\n",
        "encoded_text = tokenizer.batch_encode_plus(\n",
        "    text_data,\n",
        "    add_special_tokens=True,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        "    max_length=128  # Adjust as needed\n",
        ")\n",
        "\n",
        "input_ids = encoded_text['input_ids']\n",
        "attention_mask = encoded_text['attention_mask']"
      ],
      "metadata": {
        "id": "cVk3pb1v59bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to tensors\n",
        "y_train_text = torch.tensor(y_train)\n",
        "y_val_text = torch.tensor(y_val)\n",
        "\n",
        "# Create TensorDatasets for text data\n",
        "train_dataset = TensorDataset(input_ids, attention_mask, y_train_text)\n",
        "val_dataset = TensorDataset(input_ids, attention_mask, y_val_text)"
      ],
      "metadata": {
        "id": "FER67hSw6EOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DataLoader for batching\n",
        "train_batch_size = 16  # Adjust as needed\n",
        "val_batch_size = 16    # Adjust as needed\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "daRoxKVi6Kuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert structured data to TabularPandas format\n",
        "cont_names = list(range(combined_features.shape[1]))  # Assuming structured features are numerical\n",
        "cat_names = []  # No categorical features in this scenario\n",
        "procs = [Categorify, Normalize]\n",
        "\n",
        "# Create TabularPandas objects\n",
        "tabular_data = TabularPandas(pd.DataFrame(combined_features), procs=procs, cat_names=cat_names, cont_names=cont_names, y_names='Label')\n",
        "\n",
        "# Split data and create DataLoaders\n",
        "splits = RandomSplitter(valid_pct=0.2)(range_of(tabular_data))\n",
        "to = tabular_data.new(itemgetter(splits))\n",
        "dls = to.dataloaders(bs=64)\n"
      ],
      "metadata": {
        "id": "TNW0zkg56S6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Tabular CNN architecture\n",
        "class TabularCNN(Module):\n",
        "    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=0.5):\n",
        "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_szs])\n",
        "        self.emb_drop = nn.Dropout(ps)\n",
        "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "\n",
        "        layer_list = []\n",
        "        n_emb = sum((nf for ni, nf in emb_szs))\n",
        "        n_in = n_emb + n_cont\n",
        "\n",
        "        for n_out in layers:\n",
        "            layer_list.append(nn.Linear(n_in, n_out))\n",
        "            layer_list.append(nn.ReLU(inplace=True))\n",
        "            layer_list.append(nn.BatchNorm1d(n_out))\n",
        "            layer_list.append(nn.Dropout(ps))\n",
        "            n_in = n_out\n",
        "\n",
        "        layer_list.append(nn.Linear(layers[-1], out_sz))\n",
        "\n",
        "        self.layers = nn.Sequential(*layer_list)\n",
        "\n",
        "    def forward(self, x_cat, x_cont):\n",
        "        embeddings = []\n",
        "        for i, e in enumerate(self.embeds):\n",
        "            embeddings.append(e(x_cat[:, i]))\n",
        "        x = torch.cat(embeddings, 1)\n",
        "        x = self.emb_drop(x)\n",
        "\n",
        "        x_cont = self.bn_cont(x_cont)\n",
        "        x = torch.cat([x, x_cont], 1)\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Zu620u1s6YSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and set up the model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tabular_cnn_model = TabularCNN(emb_szs, len(cont_names), 2, [200, 100], ps=0.5)\n",
        "tabular_cnn_model.to(device)\n",
        "\n",
        "# Create FastAI Learner\n",
        "learn = Learner(dls, tabular_cnn_model, opt_func=Adam, loss_func=nn.CrossEntropyLoss(), metrics=accuracy)\n",
        "\n",
        "# Fine-tune the model\n",
        "learn.fine_tune(5, base_lr=1e-3)"
      ],
      "metadata": {
        "id": "Bwb9MMGR6u1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VI. Ensembling Models (Combining Mulitple Models)"
      ],
      "metadata": {
        "id": "srwpLB9S8gAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Voting Classifier : Combining the predictions of multiple models"
      ],
      "metadata": {
        "id": "Sg-PW_lV8oX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of models to include in the ensemble\n",
        "model_list = [('xgboost', xgb_model), ('lstm', lstm_model), ('tabular_cnn', tabular_cnn_model)]\n",
        "\n",
        "# Create the Voting Classifier\n",
        "voting_classifier = VotingClassifier(estimators=model_list, voting='soft')\n",
        "\n",
        "# Define hyperparameters to search\n",
        "params = {\n",
        "    'xgboost__n_estimators': [50, 100, 150],\n",
        "    'lstm__hidden_size': [64, 128, 256],\n",
        "    'tabular_cnn__layers': [[200, 100], [300, 150]],\n",
        "    'voting': ['soft', 'hard']\n",
        "}\n"
      ],
      "metadata": {
        "id": "n_WMWDqO8yVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=voting_classifier, param_grid=params, scoring='accuracy', cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best ensemble model\n",
        "best_ensemble_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict using the best ensemble model\n",
        "ensemble_preds = best_ensemble_model.predict(X_val)\n",
        "\n",
        "# Calculate ensemble accuracy\n",
        "ensemble_accuracy = accuracy_score(y_val, ensemble_preds)\n",
        "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Mm_eNUF483ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VII. Hyper Paramter Tuning and Optimization"
      ],
      "metadata": {
        "id": "M4GhLoz0E00T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter bounds for each model\n",
        "param_bounds = {\n",
        "    'xgboost_n_estimators': (50, 150),\n",
        "    'xgboost_max_depth': (3, 7),\n",
        "    # Add more bounds for XGBoost hyperparameters\n",
        "    'lstm_hidden_size': (64, 256),\n",
        "    'lstm_num_layers': (1, 3),\n",
        "    # Add more bounds for LSTM hyperparameters\n",
        "    'tabular_cnn_layers_0': (50, 200),\n",
        "    'tabular_cnn_layers_1': (25, 100),\n",
        "    'tabular_cnn_ps': (0.3, 0.7),\n",
        "    # Add more bounds for Tabular CNN hyperparameters\n",
        "}"
      ],
      "metadata": {
        "id": "L9-bJKZIFCw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function to maximize (accuracy)\n",
        "def objective_function(**params):\n",
        "    # Create the Voting Classifier with given hyperparameters\n",
        "    ensemble_model = VotingClassifier(estimators=model_list, voting='soft')\n",
        "\n",
        "    # Set XGBoost hyperparameters\n",
        "    ensemble_model.xgboost.n_estimators = int(params['xgboost_n_estimators'])\n",
        "    ensemble_model.xgboost.max_depth = int(params['xgboost_max_depth'])\n",
        "    # Set other XGBoost hyperparameters similarly\n",
        "\n",
        "    # Set LSTM hyperparameters\n",
        "    ensemble_model.lstm.hidden_size = int(params['lstm_hidden_size'])\n",
        "    ensemble_model.lstm.num_layers = int(params['lstm_num_layers'])\n",
        "    # Set other LSTM hyperparameters similarly\n",
        "\n",
        "    # Set Tabular CNN hyperparameters\n",
        "    ensemble_model.tabular_cnn.layers = [int(params['tabular_cnn_layers_0']), int(params['tabular_cnn_layers_1'])]\n",
        "    ensemble_model.tabular_cnn.ps = params['tabular_cnn_ps']\n",
        "    ensemble_model.tabular_cnn.learning_rate = params['tabular_cnn_learning_rate']\n",
        "    ensemble_model.tabular_cnn.dropout = params['tabular_cnn_dropout']\n",
        "    ensemble_model.tabular_cnn.batch_size = int(params['tabular_cnn_batch_size'])\n",
        "    # Set other Tabular CNN hyperparameters similarly\n",
        "\n",
        "    # Fit the model on training data\n",
        "    ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict using the model\n",
        "    ensemble_preds = ensemble_model.predict(X_val)\n",
        "\n",
        "    # Calculate and return negative accuracy (to be maximized)\n",
        "    return -accuracy_score(y_val, ensemble_preds)"
      ],
      "metadata": {
        "id": "ynSMKBd7GHLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Bayesian Optimization object\n",
        "bayes_optimizer = BayesianOptimization(f=objective_function, pbounds=param_bounds, random_state=42)\n",
        "\n",
        "# Perform optimization\n",
        "max_iter = 10  # Number of iterations\n",
        "bayes_optimizer.maximize(init_points=5, n_iter=max_iter)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = bayes_optimizer.max['params']\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ],
      "metadata": {
        "id": "gS3b3AuWGNAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIII. Neural Architectural Search (NAS)"
      ],
      "metadata": {
        "id": "ObCBKFVJHR9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the AutoKeras classifier using structured data block\n",
        "clf_nas = ak.StructuredDataClassifier(max_trials=10, overwrite=True)\n",
        "\n",
        "# Perform NAS for architecture search\n",
        "clf_nas.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best model architecture from NAS\n",
        "best_nas_model = clf_nas.export_model()"
      ],
      "metadata": {
        "id": "zomKmPzQHYok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ensemble models (XGBoost, LSTM, Tabular CNN)\n",
        "\n",
        "# XGBoost\n",
        "clf_xgboost = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "clf_lstm = Sequential()\n",
        "clf_lstm.add(LSTM(units=128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "clf_lstm.add(Dense(units=1, activation='sigmoid'))\n",
        "clf_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Tabular CNN\n",
        "tabular_cnn_layers = [64, 32]  # Example architecture\n",
        "clf_tabular_cnn = TabularCNN(input_shape=(X_train.shape[1],), layers=tabular_cnn_layers, dropout=0.2, batch_size=32)\n",
        "\n",
        "# Create a list of models for Voting Classifier\n",
        "model_list = [('xgboost', clf_xgboost), ('lstm', clf_lstm), ('tabular_cnn', clf_tabular_cnn), ('nas', best_nas_model)]"
      ],
      "metadata": {
        "id": "MtIjz-gwIB9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Bayesian Optimization\n",
        "def objective_function(**params):\n",
        "    # Create the Voting Classifier with given hyperparameters\n",
        "    ensemble_model = VotingClassifier(estimators=model_list, voting='soft')\n",
        "\n",
        "    # Set hyperparameters for XGBoost, LSTM, Tabular CNN (similar to previous steps)\n",
        "    clf_xgboost.n_estimators = int(params['xgboost_n_estimators'])\n",
        "    clf_xgboost.max_depth = int(params['xgboost_max_depth'])\n",
        "\n",
        "    clf_lstm.layers[0].units = int(params['lstm_hidden_size'])\n",
        "    clf_lstm.layers[0].activation = 'relu'\n",
        "    clf_lstm.layers[0].input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    tabular_cnn_layers[0] = int(params['tabular_cnn_layers_0'])\n",
        "    tabular_cnn_layers[1] = int(params['tabular_cnn_layers_1'])\n",
        "    clf_tabular_cnn.layers = tabular_cnn_layers\n",
        "    clf_tabular_cnn.dropout = params['tabular_cnn_dropout']\n",
        "    clf_tabular_cnn.batch_size = int(params['tabular_cnn_batch_size'])\n",
        "\n",
        "    # ... (same as previous code snippet for NAS-optimized architecture)\n",
        "\n",
        "    # Get the architecture of the best NAS model\n",
        "    best_nas_architecture = best_nas_model.get_config()\n",
        "\n",
        "    # Create a new Sequential model using the best architecture\n",
        "    clf_nas_optimized = Sequential.from_config(best_nas_architecture)\n",
        "\n",
        "    # Compile the model\n",
        "    clf_nas_optimized.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Fit the NAS-optimized model on training data\n",
        "    clf_nas_optimized.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "    # Predict using the NAS-optimized model\n",
        "    nas_optimized_preds = clf_nas_optimized.predict_classes(X_val)\n",
        "\n",
        "    # Calculate and return negative accuracy (to be maximized)\n",
        "    return -accuracy_score(y_val, nas_optimized_preds)\n",
        "\n",
        "    # Fit the model on training data\n",
        "    ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict using the model\n",
        "    ensemble_preds = ensemble_model.predict(X_val)\n",
        "\n",
        "    # Calculate and return negative accuracy (to be maximized)\n",
        "    return -accuracy_score(y_val, ensemble_preds)"
      ],
      "metadata": {
        "id": "hfedtOeAINyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter bounds for Bayesian Optimization\n",
        "param_bounds = {\n",
        "    'xgboost_n_estimators': (50, 300),\n",
        "    'xgboost_max_depth': (3, 15),\n",
        "    'lstm_hidden_size': (16, 256),\n",
        "    'lstm_num_layers': (1, 3),\n",
        "    'tabular_cnn_layers_0': (32, 128),\n",
        "    'tabular_cnn_layers_1': (16, 64),\n",
        "    'tabular_cnn_ps': (0.1, 0.5),\n",
        "    'tabular_cnn_learning_rate': (0.001, 0.01),\n",
        "    'tabular_cnn_dropout': (0.1, 0.5),\n",
        "    'tabular_cnn_batch_size': (16, 128),\n",
        "}\n",
        "\n",
        "# Create Bayesian Optimization object\n",
        "bayes_optimizer = BayesianOptimization(f=objective_function, pbounds=param_bounds, random_state=42)\n",
        "\n",
        "# Perform optimization\n",
        "max_iter = 10  # Number of iterations\n",
        "bayes_optimizer.maximize(init_points=5, n_iter=max_iter)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = bayes_optimizer.max['params']\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ],
      "metadata": {
        "id": "hDFWUpEiIR0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATING PERFORMANCE"
      ],
      "metadata": {
        "id": "KPuiWpW8LgXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters for XGBoost\n",
        "xgboost_params = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'gamma': 0,\n",
        "    'min_child_weight': 1,\n",
        "    'reg_alpha': 0,\n",
        "    'reg_lambda': 1,\n",
        "}\n",
        "\n",
        "# Set hyperparameters for LSTM\n",
        "lstm_params = {\n",
        "    'units': 64,\n",
        "    'dropout': 0.2,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 10,\n",
        "    'learning_rate': 0.001,\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'binary_crossentropy',\n",
        "}\n",
        "\n",
        "\n",
        "# Set hyperparameters for Tabular CNN\n",
        "tabular_cnn_params = {\n",
        "    'num_layers': 3,\n",
        "    'hidden_units': [128, 64, 32],\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 64,\n",
        "    'epochs': 20,\n",
        "    'learning_rate': 0.001,\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'binary_crossentropy',\n",
        "}\n",
        "\n",
        "# Fit the ensemble model on training data\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the ensemble model\n",
        "ensemble_preds = ensemble_model.predict(X_val)\n",
        "\n",
        "# Convert probabilities to binary predictions (using a threshold of 0.5)\n",
        "ensemble_preds_binary = (ensemble_preds > 0.5).astype(int)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "accuracy = accuracy_score(y_val, ensemble_preds_binary)\n",
        "precision = precision_score(y_val, ensemble_preds_binary)\n",
        "recall = recall_score(y_val, ensemble_preds_binary)\n",
        "f1 = f1_score(y_val, ensemble_preds_binary)\n",
        "conf_matrix = confusion_matrix(y_val, ensemble_preds_binary)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "id": "OAuLCnEoLjCU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}